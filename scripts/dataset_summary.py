"""
Dataset Performance Summary
Tá»•ng há»£p káº¿t quáº£ performance trÃªn cÃ¡c datasets khÃ¡c nhau
"""

import pandas as pd
import numpy as np
from pathlib import Path

def summarize_results():
    """Tá»•ng há»£p káº¿t quáº£ tá»« cÃ¡c tests Ä‘Ã£ cháº¡y"""
    
    print("ðŸ“Š DATASET PERFORMANCE SUMMARY")
    print("=" * 60)
    
    # Results tá»« 2 datasets chÃ­nh
    results = {
        "Challenging Dataset (Sophisticated)": {
            "description": "Advanced jailbreaks, edge cases, adversarial examples",
            "samples": 199,
            "malicious_ratio": 0.63,
            "performance": {
                "Logistic Regression": 0.898,
                "Random Forest": 0.925,
                "SVM": 0.828,
                "Gradient Boosting": 0.925
            },
            "key_insights": [
                "Realistic evaluation with sophisticated attacks",
                "Random Forest and Gradient Boosting perform best",
                "Contains borderline cases for robust testing",
                "Good for model development and validation"
            ]
        },
        
        "HuggingFace Dataset (Production-Scale)": {
            "description": "Large-scale real prompt injection dataset from research",
            "samples": 373646,
            "malicious_ratio": 0.235,
            "performance": {
                "Logistic Regression": 0.721,
                "Random Forest": 0.709,
                "SVM": 0.671,
                "Gradient Boosting": 0.706
            },
            "key_insights": [
                "Production-ready evaluation dataset",
                "Large-scale with 373K+ real samples",
                "Diverse attack patterns and sources",
                "Best for final model validation"
            ]
        }
    }
    
    # Print performance comparison table
    print("\nðŸŽ¯ F1 SCORE COMPARISON:")
    print("-" * 70)
    
    models = ["Logistic Regression", "Random Forest", "SVM", "Gradient Boosting"]
    datasets = list(results.keys())
    
    # Header
    print(f"{'Model':<20} {'Challenging':<12} {'HuggingFace':<12} {'Performance Gap':<15}")
    print("-" * 70)
    
    # Data rows
    for model in models:
        challenge_score = results["Challenging Dataset (Sophisticated)"]["performance"][model]
        hf_score = results["HuggingFace Dataset (Production-Scale)"]["performance"][model]
        
        gap = challenge_score - hf_score
        
        print(f"{model:<20} {challenge_score:.3f}       {hf_score:.3f}        {gap:.3f}")
    
    # Dataset characteristics
    print(f"\nðŸ“‹ DATASET CHARACTERISTICS:")
    print("-" * 80)
    
    for dataset_name, data in results.items():
        print(f"\n{dataset_name}:")
        print(f"  ðŸ“Š Samples: {data['samples']}")
        print(f"  âš–ï¸  Malicious Ratio: {data['malicious_ratio']:.0%}")
        print(f"  ðŸ“ Description: {data['description']}")
        print(f"  ðŸ’¡ Key Insights:")
        for insight in data['key_insights']:
            print(f"     â€¢ {insight}")
    
    # Performance analysis
    print(f"\nðŸ” PERFORMANCE ANALYSIS:")
    print("-" * 70)
    
    print(f"\nðŸ’š BEST PERFORMING MODELS:")
    print("  â€¢ Random Forest: Most consistent across both datasets (0.925 â†’ 0.709)")
    print("  â€¢ Logistic Regression: Best on production data (0.721 F1)")
    print("  â€¢ Gradient Boosting: Good balance on challenging data (0.925 F1)")
    
    print(f"\nðŸ“‰ PERFORMANCE GAPS:")
    challenge_avg = np.mean([results["Challenging Dataset (Sophisticated)"]["performance"][m] for m in models])
    hf_avg = np.mean([results["HuggingFace Dataset (Production-Scale)"]["performance"][m] for m in models])
    
    print(f"  â€¢ Average gap (Challenging â†’ Production): {challenge_avg - hf_avg:.3f}")
    print(f"  â€¢ Largest gap (Random Forest): {0.925 - 0.709:.3f}")
    print(f"  â€¢ Smallest gap (Logistic Regression): {0.898 - 0.721:.3f}")
    
    print(f"\nâš ï¸  KEY FINDINGS:")
    print("  1. Challenging dataset (199 samples) for development & testing")
    print("  2. HuggingFace dataset (373K samples) for production validation")
    print("  3. Performance gap shows real-world complexity")
    print("  4. Models need improvement for production deployment")
    print("  5. Focus on features that work on large-scale real data")
    
    print(f"\nðŸŽ¯ RECOMMENDATIONS:")
    print("  âœ… Use Challenging dataset for rapid development & iteration")
    print("  âœ… Use HuggingFace dataset for final validation & benchmarking")
    print("  âœ… Train models on full HuggingFace data for better performance")
    print("  âœ… Focus on Logistic Regression (best production performance)")
    print("  âœ… Improve feature engineering to close the performance gap")
    print("  âœ… Consider ensemble methods for production deployment")
    
    # Dataset files summary
    print(f"\nðŸ“ AVAILABLE DATASETS:")
    datasets_dir = Path('../datasets')
    
    print(f"\nSimple/Original:")
    print(f"  â€¢ full_dataset.csv (400 samples)")
    print(f"  â€¢ train_dataset.csv, test_dataset.csv")
    
    print(f"\nChallenging/Advanced:")
    challenging_files = list(datasets_dir.glob('challenging_*.csv'))
    for f in challenging_files[-3:]:  # Show last 3
        print(f"  â€¢ {f.name}")
    
    print(f"\nReal-World/Authentic:")
    realworld_files = list(datasets_dir.glob('realworld_*.csv'))
    for f in realworld_files[-3:]:  # Show last 3
        print(f"  â€¢ {f.name}")
    
    # Update with HuggingFace results
    print(f"\nðŸš€ HUGGINGFACE DATASET (LARGE-SCALE):")
    print("  ðŸ“Š Samples: 373,646 (10,000 tested)")
    print("  âš–ï¸  Malicious Ratio: 23.5% (balanced 50% in sample)")
    print("  ðŸ“ Description: Large-scale real prompt injection dataset")
    print("  ðŸŽ¯ Performance: F1 = 0.72 (Logistic Regression best)")
    print("  ðŸ’¡ Key Insights:")
    print("     â€¢ Shows realistic performance on large-scale data")
    print("     â€¢ Significant performance drop from synthetic to real data")
    print("     â€¢ Text length: avg 1,091 chars (much longer than synthetic)")
    print("     â€¢ Contains diverse attack patterns and sources")

    print(f"\nðŸ† FINAL COMPARISON - ALL DATASETS:")
    print("-" * 80)
    print(f"{'Dataset':<20} {'Samples':<10} {'F1 Score':<10} {'Difficulty':<15} {'Recommendation'}")
    print("-" * 80)
    dont_use = "âŒ Don't use"
    development = "âœ… Development"  
    need_more = "âš ï¸ Need more data"
    final_test = "ðŸŽ¯ Final test"
    print(f"{'Simple (Synthetic)':<20} {'400':<10} {'1.000':<10} {'Too Easy':<15} {dont_use}")
    print(f"{'Challenging':<20} {'199':<10} {'0.925':<10} {'Realistic':<15} {development}")
    print(f"{'Real-World':<20} {'65':<10} {'0.978':<10} {'Authentic':<15} {need_more}")
    print(f"{'HuggingFace':<20} {'373,646':<10} {'0.721':<10} {'Production':<15} {final_test}")
    
    print(f"\nðŸŽ¯ KEY FINDINGS - PERFORMANCE DEGRADATION:")
    print("  ðŸ“‰ Simple â†’ HuggingFace: 1.000 â†’ 0.721 (28% drop)")
    print("  ðŸ“‰ Challenging â†’ HuggingFace: 0.925 â†’ 0.721 (22% drop)")
    print("  ðŸ“Š This shows the importance of testing on real-world data!")
    
    print(f"\nâœ¨ UPDATED NEXT STEPS:")
    print("  1. âœ… COMPLETED: Downloaded large-scale dataset (373K samples)")
    print("  2. âœ… COMPLETED: Performance tested on all dataset types")
    print("  3. ðŸŽ¯ RECOMMENDATION: Use HuggingFace dataset for final evaluation")
    print("  4. ðŸ”§ IMPROVEMENT: Focus on features that work on real data")
    print("  5. ðŸ“ˆ SCALING: Train on full HuggingFace dataset for better models")

if __name__ == "__main__":
    summarize_results()
